{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In Machine Lerning, classification is used when you have labelled data and your goal is to fit categorical output from input data. \n",
    "\n",
    "This notebook covers the basic pattern for creating a map of hyper-parameters and using Grid Search CV to do a rudimentary comparison of models' performance.\n",
    "\n",
    "## Imports\n",
    "\n",
    "Imports all of the libraries for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data\n",
    "\n",
    "Two datasets are read in for the purpose of demonstrating classification algorithms: wine and iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Target names: ['class_0' 'class_1' 'class_2']\n",
      "Shape: (178, 13)\n"
     ]
    }
   ],
   "source": [
    "data_wine = load_wine()\n",
    "print(\"Feature names:\", data_wine.feature_names)\n",
    "print(\"Target names:\", data_wine.target_names)\n",
    "print(\"Shape:\", data_wine.data.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_wine.data, data_wine.target, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "Shape: (150, 4)\n"
     ]
    }
   ],
   "source": [
    "data_iris = load_iris()\n",
    "print(\"Feature names:\", data_iris.feature_names)\n",
    "print(\"Target names:\", data_iris.target_names)\n",
    "print(\"Shape:\", data_iris.data.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_iris.data, data_iris.target, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models\n",
    "\n",
    "Using GridSearch with sk-learn The pattern typically involves:\n",
    "\n",
    "1. Defining a model instance\n",
    "1. Defining a map of parameters for the gridsearch to search over\n",
    "1. Defining a GridSearch object setting parameters for things such as cross-validation and parallelization\n",
    "1. Executing the GridSearch on the training data\n",
    "1. Examining the results of the gridsearch\n",
    "\n",
    "### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      params  mean_fit_time  mean_score_time  \\\n",
      "0   {'fit_intercept': True, 'penalty': 'l1'}       0.008978         0.003719   \n",
      "1   {'fit_intercept': True, 'penalty': 'l2'}       0.002741         0.000593   \n",
      "2  {'fit_intercept': False, 'penalty': 'l1'}       0.006629         0.000202   \n",
      "3  {'fit_intercept': False, 'penalty': 'l2'}       0.001562         0.000195   \n",
      "\n",
      "   mean_test_score  \n",
      "0         0.950704  \n",
      "1         0.950704  \n",
      "2         0.950704  \n",
      "3         0.950704  \n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "parameters = {'fit_intercept': (True, False), 'penalty':('l1', 'l2')}\n",
    "clf = GridSearchCV(lr, parameters, cv=5, return_train_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "cv_results = pd.DataFrame(clf.cv_results_)\n",
    "print(cv_results[['params', 'mean_fit_time', 'mean_score_time', 'mean_test_score']].sort_values(by='mean_test_score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            params  mean_fit_time  \\\n",
      "3  {'criterion': 'entropy', 'min_samples_leaf': 1}       0.000967   \n",
      "4  {'criterion': 'entropy', 'min_samples_leaf': 3}       0.001367   \n",
      "2     {'criterion': 'gini', 'min_samples_leaf': 5}       0.000390   \n",
      "0     {'criterion': 'gini', 'min_samples_leaf': 1}       0.001562   \n",
      "1     {'criterion': 'gini', 'min_samples_leaf': 3}       0.000783   \n",
      "5  {'criterion': 'entropy', 'min_samples_leaf': 5}       0.000586   \n",
      "\n",
      "   mean_score_time  mean_test_score  \n",
      "3         0.000195         0.894366  \n",
      "4         0.000000         0.894366  \n",
      "2         0.000195         0.887324  \n",
      "0         0.000586         0.880282  \n",
      "1         0.000195         0.880282  \n",
      "5         0.000195         0.880282  \n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "parameters = {'criterion': ('gini', 'entropy'), 'min_samples_leaf':[1, 3, 5]}\n",
    "clf = GridSearchCV(dt, parameters, cv=5, return_train_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "cv_results = pd.DataFrame(clf.cv_results_)\n",
    "print(cv_results[['params', 'mean_fit_time', 'mean_score_time', 'mean_test_score']].sort_values(by='mean_test_score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       params  mean_fit_time  mean_score_time  \\\n",
      "2    {'n_neighbors': 3, 'weights': 'uniform'}       0.000584         0.000781   \n",
      "6   {'n_neighbors': 10, 'weights': 'uniform'}       0.000195         0.000772   \n",
      "0    {'n_neighbors': 1, 'weights': 'uniform'}       0.000781         0.001367   \n",
      "1   {'n_neighbors': 1, 'weights': 'distance'}       0.000388         0.000772   \n",
      "3   {'n_neighbors': 3, 'weights': 'distance'}       0.000585         0.000976   \n",
      "7  {'n_neighbors': 10, 'weights': 'distance'}       0.000391         0.000585   \n",
      "4    {'n_neighbors': 5, 'weights': 'uniform'}       0.000780         0.000195   \n",
      "5   {'n_neighbors': 5, 'weights': 'distance'}       0.000000         0.000979   \n",
      "\n",
      "   mean_test_score  \n",
      "2         0.725352  \n",
      "6         0.725352  \n",
      "0         0.711268  \n",
      "1         0.711268  \n",
      "3         0.711268  \n",
      "7         0.711268  \n",
      "4         0.690141  \n",
      "5         0.669014  \n"
     ]
    }
   ],
   "source": [
    "nbrs = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors':[1, 3, 5, 10], 'weights': ('uniform', 'distance')}\n",
    "clf = GridSearchCV(nbrs, parameters, cv=5, return_train_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "cv_results = pd.DataFrame(clf.cv_results_)\n",
    "print(cv_results[['params', 'mean_fit_time', 'mean_score_time', 'mean_test_score']].sort_values(by='mean_test_score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   param_hidden_layer_sizes param_learning_rate_init  mean_fit_time  \\\n",
      "4                       100                   0.0005       0.019715   \n",
      "8                       150                   0.0005       0.007036   \n",
      "0                        50                   0.0005       0.028889   \n",
      "3                        50                     0.01       0.003513   \n",
      "12                (100, 50)                   0.0005       0.007426   \n",
      "10                      150                    0.005       0.016594   \n",
      "2                        50                    0.005       0.004081   \n",
      "14                (100, 50)                    0.005       0.008197   \n",
      "1                        50                    0.001       0.023618   \n",
      "6                       100                    0.005       0.005261   \n",
      "\n",
      "    mean_score_time  mean_test_score  \n",
      "4          0.000195         0.387324  \n",
      "8          0.000392         0.387324  \n",
      "0          0.000000         0.373239  \n",
      "3          0.000195         0.373239  \n",
      "12         0.000391         0.373239  \n",
      "10         0.000585         0.366197  \n",
      "2          0.000195         0.359155  \n",
      "14         0.000391         0.345070  \n",
      "1          0.000195         0.330986  \n",
      "6          0.000588         0.330986  \n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(solver='sgd', learning_rate='constant')\n",
    "parameters = {'hidden_layer_sizes':[(50), (100), (150), (100, 50)], 'learning_rate_init': [0.0005, 0.001, 0.005, 0.01]}\n",
    "clf = GridSearchCV(mlp, parameters, cv=5, return_train_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "cv_results = pd.DataFrame(clf.cv_results_)\n",
    "# print(cv_results)\n",
    "print(cv_results[['param_hidden_layer_sizes', 'param_learning_rate_init', 'mean_fit_time', 'mean_score_time', 'mean_test_score']].sort_values(by='mean_test_score', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembles\n",
    "\n",
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               params param_n_estimators  \\\n",
      "23  {'max_depth': 3, 'min_samples_leaf': 1, 'n_est...                 50   \n",
      "61  {'max_depth': 10, 'min_samples_leaf': 1, 'n_es...                 10   \n",
      "47  {'max_depth': 5, 'min_samples_leaf': 3, 'n_est...                 50   \n",
      "51  {'max_depth': 5, 'min_samples_leaf': 5, 'n_est...                 50   \n",
      "26  {'max_depth': 3, 'min_samples_leaf': 3, 'n_est...                 20   \n",
      "25  {'max_depth': 3, 'min_samples_leaf': 3, 'n_est...                 10   \n",
      "22  {'max_depth': 3, 'min_samples_leaf': 1, 'n_est...                 20   \n",
      "58  {'max_depth': 5, 'min_samples_leaf': 20, 'n_es...                 20   \n",
      "63  {'max_depth': 10, 'min_samples_leaf': 1, 'n_es...                 50   \n",
      "39  {'max_depth': 3, 'min_samples_leaf': 20, 'n_es...                 50   \n",
      "\n",
      "    mean_fit_time  mean_score_time  mean_test_score  \n",
      "23       0.056811         0.002928         0.978873  \n",
      "61       0.009954         0.000976         0.978873  \n",
      "47       0.049190         0.002927         0.971831  \n",
      "51       0.050362         0.002931         0.971831  \n",
      "26       0.021863         0.001552         0.971831  \n",
      "25       0.010158         0.000781         0.971831  \n",
      "22       0.021472         0.001561         0.971831  \n",
      "58       0.021275         0.001366         0.964789  \n",
      "63       0.050362         0.002733         0.964789  \n",
      "39       0.049970         0.002733         0.964789  \n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "parameters = {'max_depth': [1, 3, 5, 10],\n",
    "              'min_samples_leaf': [1, 3, 5, 10, 20],\n",
    "              'n_estimators': [5, 10, 20, 50]}\n",
    "clf = GridSearchCV(rf, parameters, cv=5, return_train_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "cv_results = pd.DataFrame(clf.cv_results_)\n",
    "print(cv_results[['params', 'param_n_estimators', 'mean_fit_time', 'mean_score_time', 'mean_test_score']].sort_values(by='mean_test_score', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               params  \\\n",
      "15  {'base_estimator__criterion': 'gini', 'base_es...   \n",
      "9   {'base_estimator__criterion': 'gini', 'base_es...   \n",
      "8   {'base_estimator__criterion': 'gini', 'base_es...   \n",
      "26  {'base_estimator__criterion': 'entropy', 'base...   \n",
      "17  {'base_estimator__criterion': 'gini', 'base_es...   \n",
      "13  {'base_estimator__criterion': 'gini', 'base_es...   \n",
      "39  {'base_estimator__criterion': 'entropy', 'base...   \n",
      "6   {'base_estimator__criterion': 'gini', 'base_es...   \n",
      "18  {'base_estimator__criterion': 'gini', 'base_es...   \n",
      "10  {'base_estimator__criterion': 'gini', 'base_es...   \n",
      "\n",
      "   param_base_estimator__min_samples_leaf param_n_estimators  mean_fit_time  \\\n",
      "15                                     10                 50       0.087441   \n",
      "9                                       5                 10       0.015804   \n",
      "8                                       5                  5       0.007019   \n",
      "26                                      3                 20       0.029084   \n",
      "17                                     20                 10       0.017372   \n",
      "13                                     10                 10       0.014436   \n",
      "39                                     20                 50       0.076535   \n",
      "6                                       3                 20       0.024400   \n",
      "18                                     20                 20       0.025969   \n",
      "10                                      5                 20       0.030256   \n",
      "\n",
      "    mean_score_time  mean_test_score  \n",
      "15         0.003512         0.971831  \n",
      "9          0.001171         0.971831  \n",
      "8          0.000781         0.957746  \n",
      "26         0.001562         0.957746  \n",
      "17         0.001171         0.957746  \n",
      "13         0.000968         0.957746  \n",
      "39         0.004090         0.957746  \n",
      "6          0.001170         0.957746  \n",
      "18         0.001170         0.950704  \n",
      "10         0.001366         0.950704  \n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n",
    "parameters = {'base_estimator__criterion':['gini', 'entropy'], \n",
    "              'base_estimator__min_samples_leaf': [1, 3, 5, 10, 20],\n",
    "              'n_estimators': [5, 10, 20, 50]}\n",
    "clf = GridSearchCV(ada, parameters, cv=5, return_train_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "cv_results = pd.DataFrame(clf.cv_results_)\n",
    "print(cv_results[['params', 'param_base_estimator__min_samples_leaf', 'param_n_estimators', 'mean_fit_time', 'mean_score_time', 'mean_test_score']].sort_values(by='mean_test_score', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           params  mean_fit_time  \\\n",
      "24  {'min_samples_leaf': 20, 'n_estimators': 100}       0.171971   \n",
      "19  {'min_samples_leaf': 10, 'n_estimators': 100}       0.206127   \n",
      "23   {'min_samples_leaf': 20, 'n_estimators': 50}       0.086492   \n",
      "18   {'min_samples_leaf': 10, 'n_estimators': 50}       0.114584   \n",
      "17   {'min_samples_leaf': 10, 'n_estimators': 20}       0.038836   \n",
      "16   {'min_samples_leaf': 10, 'n_estimators': 10}       0.015811   \n",
      "22   {'min_samples_leaf': 20, 'n_estimators': 20}       0.029265   \n",
      "13    {'min_samples_leaf': 5, 'n_estimators': 50}       0.083929   \n",
      "15    {'min_samples_leaf': 10, 'n_estimators': 5}       0.008003   \n",
      "9    {'min_samples_leaf': 3, 'n_estimators': 100}       0.160648   \n",
      "\n",
      "    mean_score_time  mean_test_score  \n",
      "24         0.000585         0.978873  \n",
      "19         0.000978         0.971831  \n",
      "23         0.000958         0.964789  \n",
      "18         0.000585         0.964789  \n",
      "17         0.000390         0.964789  \n",
      "16         0.000391         0.964789  \n",
      "22         0.000390         0.957746  \n",
      "13         0.000195         0.950704  \n",
      "15         0.000586         0.950704  \n",
      "9          0.000391         0.950704  \n"
     ]
    }
   ],
   "source": [
    "gbm = GradientBoostingClassifier()\n",
    "parameters = {'min_samples_leaf': [1, 3, 5, 10, 20],\n",
    "              'n_estimators': [5, 10, 20, 50, 100]}\n",
    "clf = GridSearchCV(gbm, parameters, cv=5, return_train_score=True)\n",
    "clf.fit(X_train, y_train)\n",
    "cv_results = pd.DataFrame(clf.cv_results_)\n",
    "print(cv_results[['params', 'mean_fit_time', 'mean_score_time', 'mean_test_score']].sort_values(by='mean_test_score', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Rather than studing the intricacies of a particular. Creating a good model, to a certain extent, can be simplified to trying various learning algorithms and combinations hyper-parameters that configure the learning algorithms. Certain care is still needed for things such as: understanding if the input data is actually predictive of the output and whether the learning algorithm "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
