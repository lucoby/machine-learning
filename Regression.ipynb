{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "In Machine Learning, regression is used when you have labelled data and your goal is to fit a continous output to input data. Note that for this definition of regression, something like logistic regression typically falls under classification, not regression.\n",
    "\n",
    "This notebook covers creating a simple linear regression, some simple feature engineering (creating polynomial features, scaling features), regression model training and evaluation.\n",
    "\n",
    "## Imports\n",
    "\n",
    "Imports all of the libraries for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data\n",
    "\n",
    "This pulls in the Boston house-price dataset which is a commonly used dataset for regression. The attributes are:\n",
    "\n",
    "- CRIM     per capita crime rate by town\n",
    "- ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- INDUS    proportion of non-retail business acres per town\n",
    "- CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "- NOX      nitric oxides concentration (parts per 10 million)\n",
    "- RM       average number of rooms per dwelling\n",
    "- AGE      proportion of owner-occupied units built prior to 1940\n",
    "- DIS      weighted distances to five Boston employment centres\n",
    "- RAD      index of accessibility to radial highways\n",
    "- TAX      full-value property-tax rate per \\$10,000\n",
    "- PTRATIO  pupil-teacher ratio by town\n",
    "- B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "- LSTAT    % lower status of the population\n",
    "- MEDV     Median value of owner-occupied homes in \\$1000's\n",
    "\n",
    "Typically, the first 13 are used as pottential variables to predict MEDV.\n",
    "\n",
    "In addition, we split it into training and test for validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (506, 13)\n"
     ]
    }
   ],
   "source": [
    "data_boston = load_boston()\n",
    "print(\"Shape:\", data_boston.data.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_boston.data, data_boston.target, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models\n",
    "\n",
    "\n",
    "### A simple linear regression\n",
    "\n",
    "This trains a one-off simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R^2: 0.732\n",
      "Test R^2: 0.761\n",
      "A few predictions based on a simple model:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>ACTUAL MDEV</th>\n",
       "      <th>PREDICTD MDEV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.32909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.713</td>\n",
       "      <td>6.185</td>\n",
       "      <td>98.7</td>\n",
       "      <td>2.2616</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>18.13</td>\n",
       "      <td>14.1</td>\n",
       "      <td>17.674829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.15038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.581</td>\n",
       "      <td>5.856</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.9444</td>\n",
       "      <td>2.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>370.31</td>\n",
       "      <td>25.41</td>\n",
       "      <td>17.3</td>\n",
       "      <td>15.575196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.13158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.547</td>\n",
       "      <td>6.176</td>\n",
       "      <td>72.5</td>\n",
       "      <td>2.7301</td>\n",
       "      <td>6.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>393.30</td>\n",
       "      <td>12.04</td>\n",
       "      <td>21.2</td>\n",
       "      <td>23.190176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.65580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.597</td>\n",
       "      <td>5.155</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5894</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>210.97</td>\n",
       "      <td>20.08</td>\n",
       "      <td>16.3</td>\n",
       "      <td>11.524100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499</td>\n",
       "      <td>5.841</td>\n",
       "      <td>61.4</td>\n",
       "      <td>3.3779</td>\n",
       "      <td>5.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>377.56</td>\n",
       "      <td>11.41</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.553080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM   ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n",
       "0   9.32909  0.0  18.10   0.0  0.713  6.185   98.7  2.2616  24.0  666.0   \n",
       "1   0.15038  0.0  25.65   0.0  0.581  5.856   97.0  1.9444   2.0  188.0   \n",
       "2   0.13158  0.0  10.01   0.0  0.547  6.176   72.5  2.7301   6.0  432.0   \n",
       "3  28.65580  0.0  18.10   0.0  0.597  5.155  100.0  1.5894  24.0  666.0   \n",
       "4   0.09744  0.0   5.96   0.0  0.499  5.841   61.4  3.3779   5.0  279.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  ACTUAL MDEV  PREDICTD MDEV  \n",
       "0     20.2  396.90  18.13         14.1      17.674829  \n",
       "1     19.1  370.31  25.41         17.3      15.575196  \n",
       "2     17.8  393.30  12.04         21.2      23.190176  \n",
       "3     20.2  210.97  20.08         16.3      11.524100  \n",
       "4     19.2  377.56  11.41         20.0      22.553080  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model and hyper parameters\n",
    "lin_model = LinearRegression(fit_intercept=True)\n",
    "# Train the modl\n",
    "lin_model.fit(X_train, y_train)\n",
    "# Make predictions on the training data\n",
    "y_pred = lin_model.predict(X_train)\n",
    "\n",
    "# Score the model\n",
    "train_r2 = lin_model.score(X_train, y_train)\n",
    "test_r2 = lin_model.score(X_test, y_test)\n",
    "\n",
    "print((\"Training R^2: {:,.3f}\").format(train_r2))\n",
    "print((\"Test R^2: {:,.3f}\").format(test_r2))\n",
    "\n",
    "print(\"A few predictions based on a simple model:\")\n",
    "simple_results = pd.DataFrame(X_train, columns=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT'])\n",
    "simple_results['ACTUAL MDEV'] = y_train\n",
    "simple_results['PREDICTD MDEV'] = y_pred\n",
    "simple_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more complicated model\n",
    "\n",
    "The previous code creates one model at a time. This isn't very efficient if you're trying to test a variety of hyper-parameters or model configurations to see what creates the \"best\" model. Here we do a few things a bit differently to simplify and automate training and evaluating models:\n",
    "\n",
    "1. Define a pipeline that chains generating polynomial features with the linear regression model to create a polynomial regression model.\n",
    "1. Use gridsearch to test each combination of hyper-parameters.\n",
    "1. Loop through different linear regression models (that vary on the regularization used).\n",
    "\n",
    "#### Note on regularization\n",
    "\n",
    "In a linear regression model, complexity is associated with the number of non-zero coefficients and the magnitude of the coefficients. A more complicated model can fit more nuanced data, but is also more likely to overfit the training data. Regularization penalizes model complexity and serves a check on overfitting the data. A general form for L*p*-norm is:\n",
    "\n",
    "$L_p(\\mathbf{x}) = \\sqrt[p]{\\sum_i{\\left | x_i^p\\right |}}$\n",
    "\n",
    "\n",
    "More concretely:\n",
    "\n",
    "- l0 (uncommon): Number of non-zero coefficients.\n",
    "- l1-norm (common): Sum of the absolute value of coefficients. Regularizing with this is commonly referred to as lasso. \n",
    "- l2-norm (common): Square root of the sum of squares of coefficients. Regularizing with this is commonly referred to as ridge.\n",
    "- l-infinity-norm (uncommon): The maximum of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.base.LinearRegression'>\n",
      "                              params  mean_fit_time  mean_score_time  \\\n",
      "1  {'polynomialfeatures__degree': 2}       0.007115         0.002431   \n",
      "0  {'polynomialfeatures__degree': 1}       0.002324         0.000949   \n",
      "3  {'polynomialfeatures__degree': 4}       0.135308         0.048760   \n",
      "2  {'polynomialfeatures__degree': 3}       0.055854         0.010824   \n",
      "\n",
      "   mean_test_score  \n",
      "1         0.732345  \n",
      "0         0.717943  \n",
      "3     -1441.482409  \n",
      "2     -1597.457245  \n",
      "<class 'sklearn.linear_model.coordinate_descent.Lasso'>\n",
      "                              params  mean_fit_time  mean_score_time  \\\n",
      "1  {'polynomialfeatures__degree': 2}       0.037231         0.003431   \n",
      "0  {'polynomialfeatures__degree': 1}       0.001051         0.000791   \n",
      "2  {'polynomialfeatures__degree': 3}       0.199968         0.011112   \n",
      "3  {'polynomialfeatures__degree': 4}       0.881153         0.033844   \n",
      "\n",
      "   mean_test_score  \n",
      "1         0.828389  \n",
      "0         0.669705  \n",
      "2         0.368445  \n",
      "3        -2.048730  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CobyLU\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:152: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number1.686867e-19\n",
      "  overwrite_a=False)\n",
      "C:\\Users\\CobyLU\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:152: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number1.813703e-19\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.ridge.Ridge'>\n",
      "                              params  mean_fit_time  mean_score_time  \\\n",
      "1  {'polynomialfeatures__degree': 2}       0.003511         0.002150   \n",
      "0  {'polynomialfeatures__degree': 1}       0.001305         0.000481   \n",
      "2  {'polynomialfeatures__degree': 3}       0.040669         0.010342   \n",
      "3  {'polynomialfeatures__degree': 4}       0.095353         0.044213   \n",
      "\n",
      "   mean_test_score  \n",
      "1         0.821751  \n",
      "0         0.714930  \n",
      "2        -5.665710  \n",
      "3      -180.263430  \n"
     ]
    }
   ],
   "source": [
    "def PolynomialRegression(degree=2, linear_model=LinearRegression(), **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), linear_model(**kwargs))\n",
    "\n",
    "def fit_model(X, y):\n",
    "    # Create cross-validation sets from the training data\n",
    "    cv_sets = ShuffleSplit(n_splits=10, test_size = 0.20, random_state = 0)\n",
    " \n",
    "    #Creating a dictionary for the parameter degree with a range from 1 to 10 and fit_intercept of true or false\n",
    "    params = {'polynomialfeatures__degree': [1,2,3,4]}\n",
    "\n",
    "    best_models = []\n",
    "    for m in [LinearRegression, Lasso, Ridge]:\n",
    "        grid = GridSearchCV(PolynomialRegression(linear_model=m), param_grid = params, \n",
    "                         cv=cv_sets, \n",
    "                         scoring='r2',\n",
    "                         return_train_score=True)\n",
    "        grid = grid.fit(X, y)\n",
    "        cv_results = pd.DataFrame(grid.cv_results_)\n",
    "        print(m)\n",
    "        print(cv_results[['params', 'mean_fit_time', 'mean_score_time', 'mean_test_score']].sort_values(by='mean_test_score', ascending=False))\n",
    "        best_models.append(grid.best_estimator_)\n",
    "    \n",
    "    return best_models\n",
    "\n",
    "#Fitting the training data using grid search\n",
    "models = fit_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model coefficients with regularization\n",
    "\n",
    "Here's a bit of code to look at the coefficients produced by lasso and ridge regularization to illustrate that is is, in fact, reducing the magnitude of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.452603718075174\n",
      "[ 0.         -0.          0.         -0.          0.         -0.\n",
      "  0.         -0.          0.         -0.         -2.42932061 -0.\n",
      "  0.         -7.8146962 ]\n",
      "-0.01674592237683825\n"
     ]
    }
   ],
   "source": [
    "print(models[0].named_steps['linearregression'].coef_.mean())\n",
    "print(models[1].named_steps['lasso'].coef_)\n",
    "print(models[2].named_steps['ridge'].coef_.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Our attributes can exist on radically different scales. i.e. number of rooms might on the order of 1-10 and the of the house could be on the order of 1-100. Being on the extreme one of the ranges could be useful to our regression model, but the regularization on the coefficients biases towards giving more importance to attributes that typically have large values i.e. biasing towards using age over number of rooms.\n",
    "\n",
    "Here we apply a min-max scaler to scale the different attributes. The min-max scaler scales each attribute so the minimum is 0 and the maximum is 1. A reasonable alternative could be to normalize the variables which sets the mean to 0 and the standard deviation to 1. Variables may still skew if the data isn't normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CRIM    ZN     INDUS  CHAS       NOX        RM       AGE       DIS  \\\n",
      "0  0.000000  0.18  0.067815   0.0  0.314815  0.577505  0.641607  0.269203   \n",
      "1  0.000236  0.00  0.242302   0.0  0.172840  0.547998  0.782698  0.348962   \n",
      "2  0.000236  0.00  0.242302   0.0  0.172840  0.694386  0.599382  0.348962   \n",
      "3  0.000293  0.00  0.063050   0.0  0.150206  0.658555  0.441813  0.448545   \n",
      "4  0.000705  0.00  0.063050   0.0  0.150206  0.687105  0.528321  0.448545   \n",
      "\n",
      "        RAD       TAX   PTRATIO         B     LSTAT  \n",
      "0  0.000000  0.208015  0.287234  1.000000  0.089680  \n",
      "1  0.043478  0.104962  0.553191  1.000000  0.204470  \n",
      "2  0.043478  0.104962  0.553191  0.989737  0.063466  \n",
      "3  0.086957  0.066794  0.648936  0.994276  0.033389  \n",
      "4  0.086957  0.066794  0.648936  1.000000  0.099338  \n"
     ]
    }
   ],
   "source": [
    "features_transformed = pd.DataFrame(data=data_boston.data, columns=data_boston.feature_names)\n",
    "\n",
    "# Applying scaling using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "features_transformed[data_boston.feature_names] = scaler.fit_transform(features_transformed)\n",
    "print(features_transformed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering impact on model perforamance\n",
    "\n",
    "Scaling and normalizing variables **should** make better models. In practice, it doesn't always work. When conducting an initial investigation, with the goal of getting the \"most accurate\" model, a common practice is to build simple models first. The simple model doesn't need things like feature engineering or scaling, but it provids a baseline to compare against more \"correct\" or advanced models' performance. In an agile sense, it can also act as a potential MVP (minimum viable product) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.base.LinearRegression'>\n",
      "                              params  mean_fit_time  mean_score_time  \\\n",
      "1  {'polynomialfeatures__degree': 2}       0.006915         0.002533   \n",
      "0  {'polynomialfeatures__degree': 1}       0.004193         0.001001   \n",
      "3  {'polynomialfeatures__degree': 4}       0.137997         0.049908   \n",
      "2  {'polynomialfeatures__degree': 3}       0.055528         0.013584   \n",
      "\n",
      "   mean_test_score  \n",
      "1         0.751177  \n",
      "0         0.725487  \n",
      "3       -10.466120  \n",
      "2       -22.506637  \n",
      "<class 'sklearn.linear_model.coordinate_descent.Lasso'>\n",
      "                              params  mean_fit_time  mean_score_time  \\\n",
      "0  {'polynomialfeatures__degree': 1}       0.002511         0.000873   \n",
      "2  {'polynomialfeatures__degree': 3}       0.021671         0.010345   \n",
      "3  {'polynomialfeatures__degree': 4}       0.106638         0.041869   \n",
      "1  {'polynomialfeatures__degree': 2}       0.005462         0.002647   \n",
      "\n",
      "   mean_test_score  \n",
      "0         0.292238  \n",
      "2         0.290059  \n",
      "3         0.289966  \n",
      "1         0.289425  \n",
      "<class 'sklearn.linear_model.ridge.Ridge'>\n",
      "                              params  mean_fit_time  mean_score_time  \\\n",
      "3  {'polynomialfeatures__degree': 4}       0.083167         0.044697   \n",
      "2  {'polynomialfeatures__degree': 3}       0.021830         0.009809   \n",
      "1  {'polynomialfeatures__degree': 2}       0.004780         0.002260   \n",
      "0  {'polynomialfeatures__degree': 1}       0.001872         0.000768   \n",
      "\n",
      "   mean_test_score  \n",
      "3         0.844470  \n",
      "2         0.833279  \n",
      "1         0.811083  \n",
      "0         0.725303  \n"
     ]
    }
   ],
   "source": [
    "X_train_scale, X_test_scale, y_train_scale, y_test_scale = train_test_split(features_transformed, \n",
    "                    data_boston.target, test_size = 0.2, random_state = 0)\n",
    "models = fit_model(X_train_scale, y_train_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it together\n",
    "\n",
    "Here's a block of code that has the scaling and model training all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
      "0 -0.417713  0.284830 -1.287909 -0.272599 -0.144217  0.413672 -0.120013   \n",
      "1 -0.415269 -0.487722 -0.593381 -0.272599 -0.740262  0.194274  0.367166   \n",
      "2 -0.415272 -0.487722 -0.593381 -0.272599 -0.740262  1.282714 -0.265812   \n",
      "3 -0.414680 -0.487722 -1.306878 -0.272599 -0.835284  1.016303 -0.809889   \n",
      "4 -0.410409 -0.487722 -1.306878 -0.272599 -0.835284  1.228577 -0.511180   \n",
      "\n",
      "        DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
      "0  0.140214 -0.982843 -0.666608 -1.459000  0.441052 -1.075562  \n",
      "1  0.557160 -0.867883 -0.987329 -0.303094  0.441052 -0.492439  \n",
      "2  0.557160 -0.867883 -0.987329 -0.303094  0.396427 -1.208727  \n",
      "3  1.077737 -0.752922 -1.106115  0.113032  0.416163 -1.361517  \n",
      "4  1.077737 -0.752922 -1.106115  0.113032  0.441052 -1.026501  \n",
      "<class 'sklearn.linear_model.base.LinearRegression'>\n",
      "                              params  mean_fit_time  mean_score_time  \\\n",
      "1  {'polynomialfeatures__degree': 2}       0.007109         0.002342   \n",
      "0  {'polynomialfeatures__degree': 1}       0.003235         0.000728   \n",
      "2  {'polynomialfeatures__degree': 3}       0.061123         0.010121   \n",
      "3  {'polynomialfeatures__degree': 4}       0.138962         0.043554   \n",
      "\n",
      "   mean_test_score  \n",
      "1         0.752541  \n",
      "0         0.725487  \n",
      "2      -110.423222  \n",
      "3      -319.735255  \n",
      "<class 'sklearn.linear_model.coordinate_descent.Lasso'>\n",
      "                              params  mean_fit_time  mean_score_time  \\\n",
      "1  {'polynomialfeatures__degree': 2}       0.003906         0.002258   \n",
      "3  {'polynomialfeatures__degree': 4}       0.434415         0.036044   \n",
      "2  {'polynomialfeatures__degree': 3}       0.034745         0.010254   \n",
      "0  {'polynomialfeatures__degree': 1}       0.001565         0.000485   \n",
      "\n",
      "   mean_test_score  \n",
      "1         0.733124  \n",
      "3         0.698656  \n",
      "2         0.674818  \n",
      "0         0.661900  \n",
      "<class 'sklearn.linear_model.ridge.Ridge'>\n",
      "                              params  mean_fit_time  mean_score_time  \\\n",
      "1  {'polynomialfeatures__degree': 2}       0.004389         0.002221   \n",
      "0  {'polynomialfeatures__degree': 1}       0.002092         0.000580   \n",
      "2  {'polynomialfeatures__degree': 3}       0.028924         0.013222   \n",
      "3  {'polynomialfeatures__degree': 4}       0.085092         0.040764   \n",
      "\n",
      "   mean_test_score  \n",
      "1         0.808667  \n",
      "0         0.725393  \n",
      "2         0.719826  \n",
      "3       -13.934554  \n"
     ]
    }
   ],
   "source": [
    "features_transformed = pd.DataFrame(data=data_boston.data, columns=data_boston.feature_names)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_transformed[data_boston.feature_names] = scaler.fit_transform(features_transformed)\n",
    "print(features_transformed.head())\n",
    "\n",
    "X_train_scale, X_test_scale, y_train_scale, y_test_scale = train_test_split(features_transformed, \n",
    "                    data_boston.target, test_size = 0.2, random_state = 0)\n",
    "models = fit_model(X_train_scale, y_train_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
